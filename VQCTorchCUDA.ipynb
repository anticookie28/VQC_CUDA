{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0878ba8d-1c4a-43a8-8b43-71680f86c1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete! Ready for training.\n",
      "Epoch [1/50], Loss: 0.6043,Train Accuracy: 86.81%\n",
      "Epoch [2/50], Loss: 0.5646,Train Accuracy: 93.66%\n",
      "Epoch [3/50], Loss: 0.5578,Train Accuracy: 92.50%\n",
      "Epoch [4/50], Loss: 0.5525,Train Accuracy: 93.53%\n",
      "Epoch [5/50], Loss: 0.5491,Train Accuracy: 94.38%\n",
      "Epoch [6/50], Loss: 0.5472,Train Accuracy: 94.06%\n",
      "Epoch [7/50], Loss: 0.5435,Train Accuracy: 94.88%\n",
      "Epoch [8/50], Loss: 0.5422,Train Accuracy: 94.75%\n",
      "Epoch [9/50], Loss: 0.5410,Train Accuracy: 94.66%\n",
      "Epoch [10/50], Loss: 0.5406,Train Accuracy: 94.59%\n",
      "Epoch [11/50], Loss: 0.5405,Train Accuracy: 93.72%\n",
      "Epoch [12/50], Loss: 0.5393,Train Accuracy: 94.66%\n",
      "Epoch [13/50], Loss: 0.5383,Train Accuracy: 94.78%\n",
      "Epoch [14/50], Loss: 0.5379,Train Accuracy: 94.84%\n",
      "Epoch [15/50], Loss: 0.5371,Train Accuracy: 94.88%\n",
      "Epoch [16/50], Loss: 0.5371,Train Accuracy: 94.72%\n",
      "Epoch [17/50], Loss: 0.5364,Train Accuracy: 95.03%\n",
      "Epoch [18/50], Loss: 0.5366,Train Accuracy: 94.41%\n",
      "Epoch [19/50], Loss: 0.5366,Train Accuracy: 94.31%\n",
      "Epoch [20/50], Loss: 0.5364,Train Accuracy: 94.88%\n",
      "Epoch [21/50], Loss: 0.5359,Train Accuracy: 94.97%\n",
      "Epoch [22/50], Loss: 0.5357,Train Accuracy: 94.72%\n",
      "Epoch [23/50], Loss: 0.5358,Train Accuracy: 94.72%\n",
      "Epoch [24/50], Loss: 0.5358,Train Accuracy: 94.69%\n",
      "Epoch [25/50], Loss: 0.5364,Train Accuracy: 94.53%\n",
      "Epoch [26/50], Loss: 0.5369,Train Accuracy: 94.50%\n",
      "Epoch [27/50], Loss: 0.5357,Train Accuracy: 94.56%\n",
      "Epoch [28/50], Loss: 0.5366,Train Accuracy: 94.66%\n",
      "Epoch [29/50], Loss: 0.5354,Train Accuracy: 94.84%\n",
      "Epoch [30/50], Loss: 0.5351,Train Accuracy: 94.69%\n",
      "Epoch [31/50], Loss: 0.5355,Train Accuracy: 94.50%\n",
      "Epoch [32/50], Loss: 0.5367,Train Accuracy: 94.16%\n",
      "Epoch [33/50], Loss: 0.5354,Train Accuracy: 94.97%\n",
      "Epoch [34/50], Loss: 0.5355,Train Accuracy: 94.78%\n",
      "Epoch [35/50], Loss: 0.5352,Train Accuracy: 94.47%\n",
      "Epoch [36/50], Loss: 0.5367,Train Accuracy: 94.50%\n",
      "Epoch [37/50], Loss: 0.5355,Train Accuracy: 94.81%\n",
      "Epoch [38/50], Loss: 0.5354,Train Accuracy: 94.47%\n",
      "Epoch [39/50], Loss: 0.5358,Train Accuracy: 94.56%\n",
      "Epoch [40/50], Loss: 0.5352,Train Accuracy: 94.50%\n",
      "Epoch [41/50], Loss: 0.5360,Train Accuracy: 94.69%\n",
      "Epoch [42/50], Loss: 0.5355,Train Accuracy: 94.62%\n",
      "Epoch [43/50], Loss: 0.5354,Train Accuracy: 95.00%\n",
      "Epoch [44/50], Loss: 0.5353,Train Accuracy: 94.69%\n",
      "Epoch [45/50], Loss: 0.5357,Train Accuracy: 94.59%\n",
      "Epoch [46/50], Loss: 0.5358,Train Accuracy: 94.69%\n",
      "Epoch [47/50], Loss: 0.5351,Train Accuracy: 94.81%\n",
      "Epoch [48/50], Loss: 0.5357,Train Accuracy: 94.25%\n",
      "Epoch [49/50], Loss: 0.5351,Train Accuracy: 94.72%\n",
      "Epoch [50/50], Loss: 0.5354,Train Accuracy: 94.22%\n",
      "Test Accuracy: 87.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Operators:\n",
    "    @staticmethod\n",
    "    def PauliX():\n",
    "        return torch.tensor([[0, 1], [1, 0]], dtype=torch.complex64, device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def PauliY():\n",
    "        return torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex64, device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def PauliZ():\n",
    "        return torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64, device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def Rx(theta):\n",
    "        return torch.cos(theta / 2) * torch.eye(2, dtype=torch.complex64, device=device) - \\\n",
    "               1j * torch.sin(theta / 2) * Operators.PauliX()\n",
    "\n",
    "    @staticmethod\n",
    "    def Ry(theta):\n",
    "        return torch.cos(theta / 2) * torch.eye(2, dtype=torch.complex64, device=device) - \\\n",
    "               1j * torch.sin(theta / 2) * Operators.PauliY()\n",
    "\n",
    "    @staticmethod\n",
    "    def Rz(theta):\n",
    "        return torch.cos(theta / 2) * torch.eye(2, dtype=torch.complex64, device=device) - \\\n",
    "               1j * torch.sin(theta / 2) * Operators.PauliZ()\n",
    "\n",
    "    @staticmethod\n",
    "    def CNOT():\n",
    "        I = torch.eye(2, dtype=torch.complex64, device=device)\n",
    "        zero_proj = torch.tensor([[1, 0], [0, 0]], dtype=torch.complex64, device=device)\n",
    "        one_proj = torch.tensor([[0, 0], [0, 1]], dtype=torch.complex64, device=device)\n",
    "        return torch.kron(zero_proj, I) + torch.kron(one_proj, Operators.PauliX())\n",
    "\n",
    "class utils:\n",
    "    def apply_one_site(site, state, op, L=32):\n",
    "        state = state.reshape(2**(site), 2, 2**(L-site-1))\n",
    "        state = torch.tensordot(op, state, dims=([1], [1]))\n",
    "        state = state.permute(1, 0, 2).contiguous()\n",
    "        return state.reshape(-1,)\n",
    "\n",
    "    def apply_two_site(sites, state, op, L=32):\n",
    "        state = state.reshape(2**sites[0], 2, 2**(sites[1]-sites[0]-1), 2, 2**(L-sites[1]-1))\n",
    "        op = op.view(2, 2, 2, 2)\n",
    "        state = torch.tensordot(op, state, dims=([2, 3], [1, 3]))\n",
    "        state = state.permute(2, 0, 3, 1, 4).contiguous()\n",
    "        return state.reshape(-1,)\n",
    "\n",
    "    def measure_expectation(state, op, site, L=32):\n",
    "        state = state.view(2**(site), 2, 2**(L-site-1))\n",
    "        measured_state = torch.tensordot(op, state, dims=([1], [1]))\n",
    "        expectation_value = torch.vdot(state.view(-1), measured_state.view(-1)).real\n",
    "        return expectation_value\n",
    "\n",
    "def layer(params, state, L=32):\n",
    "    for i in range(L):\n",
    "        Ry_gate = Operators.Ry(params[i])\n",
    "        state = utils.apply_one_site(i, state, Ry_gate, L)\n",
    "    for i in range(L-1):\n",
    "        state = utils.apply_two_site([i, i+1], state, Operators.CNOT(), L)\n",
    "    return state\n",
    "\n",
    "def circuit(params, state, L=32):\n",
    "    n_layers = params.shape[0]\n",
    "    for i in range(n_layers):\n",
    "        state = layer(params[i], state, L=L)\n",
    "    return utils.measure_expectation(state, Operators.PauliZ(), site=7, L=L)\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from PIL import Image\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "images, labels = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "indices_2 = np.where(labels == 2)[0][:2000]\n",
    "indices_3 = np.where(labels == 3)[0][:2000]\n",
    "selected_indices = np.concatenate((indices_2, indices_3))\n",
    "selected_images = images[selected_indices]\n",
    "selected_labels = labels[selected_indices]\n",
    "\n",
    "selected_labels = np.where(selected_labels == 2, -1, 1)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image.reshape(28, 28)\n",
    "    image_resized = Image.fromarray(np.uint8(image))\n",
    "    image_resized = image_resized.resize((16, 16), Image.Resampling.LANCZOS)\n",
    "    image_resized = np.array(image_resized).reshape(-1,)\n",
    "    return image_resized / np.linalg.norm(image_resized)\n",
    "\n",
    "selected_images = np.array([preprocess_image(img) for img in selected_images])\n",
    "\n",
    "X_data = torch.tensor(selected_images, dtype=torch.float32, device=device)\n",
    "Y_data = torch.tensor(selected_labels, dtype=torch.float32, device=device)\n",
    "\n",
    "train_size = int(0.8 * len(X_data))\n",
    "X_train, X_test = X_data[:train_size], X_data[train_size:]\n",
    "Y_train, Y_test = Y_data[:train_size], Y_data[train_size:]\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train, Y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test, Y_test)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data preprocessing complete! Ready for training.\")\n",
    "\n",
    "class QuantumCircuitModel(nn.Module):\n",
    "    def __init__(self, n_layers=32, n_qubits=8):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = n_qubits\n",
    "        self.params = nn.Parameter(torch.rand(n_layers, n_qubits) * torch.pi*0.01)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, input_dim = x.shape  \n",
    "        assert input_dim == 2**self.n_qubits, f\"Expected input_dim={2**self.n_qubits}, but got {input_dim}\"\n",
    "    \n",
    "\n",
    "        state = x.to(dtype=torch.complex64,device=x.device) \n",
    "\n",
    "        results = torch.vmap(lambda s: circuit(self.params, s, L=self.n_qubits))(state)\n",
    "    \n",
    "        return results \n",
    "\n",
    "\n",
    "model = QuantumCircuitModel().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def convert_labels(y):\n",
    "    return (y + 1) / 2  \n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train, total_train = 0, 0 \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = convert_labels(labels)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  \n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        correct_train += (predictions == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f},Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = convert_labels(labels)  \n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs) > 0.5  \n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e17c37-148b-4ecd-bac9-81353bcdef6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adc79ff-2a65-4807-9435-ee89484d97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helstrom Bound: 0.5031\n"
     ]
    }
   ],
   "source": [
    "def circuit2(params, state, L=8):\n",
    "    n_layers = params.shape[0]\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        state = layer(params[i], state, L=L)\n",
    "    probabilities = torch.abs(state) ** 2  \n",
    "\n",
    "    probabilities = probabilities / torch.sum(probabilities)\n",
    "\n",
    "    return probabilities \n",
    "\n",
    "trained_params = model.params.detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    quantum_states = torch.zeros((X_data.shape[0], 2**8), dtype=torch.complex64, device=device)\n",
    "    quantum_states[:, :2**8] = X_data\n",
    "    quantum_states /= torch.norm(quantum_states, dim=1, keepdim=True)  # Normalize batch-wise\n",
    "\n",
    "    batch_probabilities = torch.vmap(lambda s: circuit2(trained_params, s, L=8))(quantum_states)\n",
    "\n",
    "    mask_class_minus1 = (Y_data == -1)\n",
    "    mask_class_plus1 = (Y_data == 1)\n",
    "\n",
    "    state_list_class_minus1 = batch_probabilities[mask_class_minus1]\n",
    "    state_list_class_plus1 = batch_probabilities[mask_class_plus1]\n",
    "\n",
    "\n",
    "assert state_list_class_minus1.numel() > 0, \"Error: No states found for class -1!\"\n",
    "assert state_list_class_plus1.numel() > 0, \"Error: No states found for class +1!\"\n",
    "\n",
    "\n",
    "state_minus = torch.mean(state_list_class_minus1, dim=0) \n",
    "state_plus = torch.mean(state_list_class_plus1, dim=0)   \n",
    "\n",
    "rho_0 = state_minus[:, None] * state_minus.conj()[None, :] \n",
    "rho_1 = state_plus[:, None] * state_plus.conj()[None, :]\n",
    "\n",
    "rho_diff = rho_0 - rho_1\n",
    "eigenvalues = torch.linalg.eigvalsh(rho_diff) \n",
    "trace_distance = 0.5 * torch.sum(torch.abs(eigenvalues))  \n",
    "\n",
    "helstrom_bound = 0.5 + 0.5 * trace_distance.item()\n",
    "print(f\"Helstrom Bound: {helstrom_bound:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01477424-f872-4963-8282-93ff2e024268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mnist2 = []\n",
    "Mnist3 = []\n",
    "for i in range(0,25):\n",
    "    test = X_data[i].cpu()\n",
    "    test /= torch.norm(test)\n",
    "    rho = torch.outer(test,test.conj())\n",
    "    Mnist2.append(rho)\n",
    "for i in range(2000,2025):\n",
    "    test = X_data[i].cpu()\n",
    "    test /= torch.norm(test)\n",
    "    rho = torch.outer(test,test.conj())\n",
    "    Mnist3.append(rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb77643-cbe6-42e8-a58c-b911cb44c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helstrom Bound: 0.8379\n"
     ]
    }
   ],
   "source": [
    "rho2 = torch.mean(torch.stack(Mnist2),dim=0)\n",
    "rho3 = torch.mean(torch.stack(Mnist3),dim=0)\n",
    "rho_diff = rho2-rho3\n",
    "eigenvalues = torch.linalg.eigvalsh(rho_diff) \n",
    "trace_distance = 0.5 * torch.sum(torch.abs(eigenvalues)) \n",
    "helstrom_bound = 0.5 + 0.5 * trace_distance.item()\n",
    "print(f\"Helstrom Bound: {helstrom_bound:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2d790-d42c-49a0-a036-8e8fe7ad41c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2933a-401f-4548-ac89-b7229c9c3e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
